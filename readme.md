# Hindi SentencePiece Tokenizer

A production-ready SentencePiece tokenizer specifically designed and optimized for the Hindi language using the Devanagari script.

## ğŸŒŸ Features

- ğŸ‡®ğŸ‡³ **Hindi-optimized**: Specifically designed for Devanagari script and Hindi language patterns
- ğŸš€ **High Performance**: Fast encoding/decoding with caching support
- ğŸ“Š **Comprehensive Evaluation**: Built-in metrics and analysis tools
- ğŸ”§ **Production Ready**: Docker support, API server, and monitoring
- ğŸ§ª **Well Tested**: Comprehensive test suite with CI/CD
- ğŸ“š **Easy Integration**: Compatible with popular ML frameworks

## ğŸš€ Quick Start

### Installation

```bash
git clone https://github.com/yourusername/hindi-sentencepiece-tokenizer.git
cd hindi-sentencepiece-tokenizer
pip install -r requirements.txt
```

### Basic Usage

```python
from hindi_tokenizer import HindiTokenizer

# Initialize tokenizer
tokenizer = HindiTokenizer()

# Train on your Hindi corpus
tokenizer.train(
    input_file="hindi_corpus.txt",
    model_name="hindi_tokenizer",
    vocab_size=32000
)

# Use the tokenizer
text = "à¤¯à¤¹ à¤à¤• à¤¹à¤¿à¤‚à¤¦à¥€ à¤µà¤¾à¤•à¥à¤¯ à¤¹à¥ˆà¥¤"
tokens = tokenizer.encode(text)
pieces = tokenizer.encode_as_pieces(text)
decoded = tokenizer.decode(tokens)

print(f"Original: {text}")
print(f"Tokens: {tokens}")
print(f"Pieces: {pieces}")
print(f"Decoded: {decoded}")
```

### Command Line Usage

```bash
# Train a new model
python -m hindi_tokenizer train --input data/hindi_corpus.txt --vocab-size 32000

# Encode text
python -m hindi_tokenizer encode --text "à¤†à¤ªà¤•à¤¾ à¤¹à¤¿à¤‚à¤¦à¥€ à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤¯à¤¹à¤¾à¤"

# Evaluate model
python -m hindi_tokenizer evaluate --test-file data/test_hindi.txt
```

## ğŸ“ Repository Structure

```
hindi-sentencepiece-tokenizer/
â”œâ”€â”€ hindi_tokenizer/           # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core.py               # Core tokenizer implementation
â”‚   â”œâ”€â”€ trainer.py            # Training utilities
â”‚   â”œâ”€â”€ evaluator.py          # Evaluation tools
â”‚   â”œâ”€â”€ preprocessor.py       # Hindi text preprocessing
â”‚   â””â”€â”€ utils.py              # Helper utilities
â”œâ”€â”€ api/                      # REST API
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ server.py            # FastAPI server
â”‚   â””â”€â”€ models.py            # API data models
â”œâ”€â”€ scripts/                  # Utility scripts
â”‚   â”œâ”€â”€ download_data.py     # Download Hindi datasets
â”‚   â”œâ”€â”€ prepare_corpus.py    # Corpus preparation
â”‚   â””â”€â”€ benchmark.py         # Performance benchmarking
â”œâ”€â”€ tests/                   # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_tokenizer.py    # Core tests
â”‚   â”œâ”€â”€ test_trainer.py      # Training tests
â”‚   â””â”€â”€ test_api.py          # API tests
â”œâ”€â”€ data/                    # Sample data
â”‚   â”œâ”€â”€ sample_hindi.txt
â”‚   â””â”€â”€ test_sentences.txt
â”œâ”€â”€ models/                  # Trained models
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ usage.md
â”‚   â”œâ”€â”€ training.md
â”‚   â””â”€â”€ api.md
â”œâ”€â”€ docker/                  # Docker files
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ .github/                 # GitHub workflows
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ setup.py                # Package setup
â”œâ”€â”€ pyproject.toml          # Modern Python packaging
â”œâ”€â”€ .gitignore              # Git ignore rules
â”œâ”€â”€ .pre-commit-config.yaml # Pre-commit hooks
â””â”€â”€ LICENSE                 # MIT License
```
